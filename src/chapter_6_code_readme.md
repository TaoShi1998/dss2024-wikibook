# CRATE demo

# Masked Completion via Structured Diffusion with White-Box Transformers

**Revised** Official demo for visualizing attention maps for CRATE Masked Auto-encoder.
* Github: https://github.com/Ma-Lab-Berkeley/CRATE
* Paper link: https://openreview.net/forum?id=PvyOYleymy

The original official demo is **NOT** available at this time. I revised it to ensure that it is runable and I changed the figures used for visualization.
## Guidlines:
* Download this notebook (*crate_demo.ipynb*) and upload it to your own colab
* Download the checkpoints by hand and put it in the right path.
* Setup colab gpu: Edit -> Notebook settings-> T4 GPU -> Save
* Run the notebook!

You can change the visilization figures by replacing images in **demo** and **pca_source** folder like I did.